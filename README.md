The problem which we are addressing involves taking real-time feedback from the facial expressions of a person on edge devices. Currently, we plan to use it as a tool for end-users to analyse the emotions in any image on a per face basis and also implement the same in real-time using the cameras feed of the android device.

All the processing happens on the device, and the application is not dependent on an internet connection.

With this, we aim to provide a platform for daily users who as of now rely on conventional methods of rehearsing and practising in front of mirrors actually to improve their presentation skills and perform better in situations like interviews, public speaking, acting and auditions.

For the regular user, the applications will serve as a tool to ensure that every photograph is perfect and communicates the right emotions, before uploading them to social media platforms. This works exceptionally well, for group photos where it becomes a tedious task to sift through multiple faces in an image to ensure quality.

API Used
- Android
  - MLkit API
  - Tensorflow Lite API
  - Android Jetpack Architectural Components
  - CameraView API
- Deep Learning
  - Tensorflow API
  - Keras Backend API
  - Open-CV API

Collab Notebbok [link](https://colab.research.google.com/drive/1YLeaN1cjrDet-cmpkMKXXn0H-0s2nQXz?usp=sharing)

Made by : [Shivam Sood](https://github.com/shivamsoods) & [Prajwal Sood](https://github.com/PrajwalSood)
